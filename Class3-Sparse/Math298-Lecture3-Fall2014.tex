\documentclass[mathserif,handout]{beamer} % Math 298 Fall 2014
\usepackage{mybeamertheme}
\usepackage{pdfsync} % works only on TeXShop, comment otherwise
\usepackage{pgf}
\usepackage{multimedia} % to embed movies in the PDF file
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{pgfpages}
\usepackage{topcapt}
\usepackage{booktabs}

\mode<presentation>
{
  \usetheme{Madrid}   %  \usetheme{Warsaw}
  % Note: this next command makes nice slides, but Adobe doesn't like it (you get blank pages)
  \setbeamertemplate{background canvas}[vertical shading][bottom=white,top=structure.fg!15]
  %\setbeamertemplate{footline}{\pgfuseimage{UCM-logo}}

  %\usecolortheme{seahorse}
  \usecolortheme{beaver}
 % \setbeamercovered{transparent}
}
%\usepackage[T1]{fontenc}
% For printing purposes only
%\pgfpagesuselayout{4 on 1}[letterpaper,border shrink=5mm,landscape]
\pgfpagesuselayout{resize to}[letterpaper,border shrink=5mm,landscape]  % print out full size on paper

\usepackage{amssymb,amsmath}
\hypersetup{%
 pdftitle={Math298 Lecture 3},
 pdfauthor={Juan Meza(UC Merced)},
 pdfsubject={Fundamental Concepts in Computational and Applied Mathematics},
 pdfkeywords={linear algebra, sparse, iterative}
 }
 
\title[Math 298 Lecture 3]{Fundamental Concepts in \\ Computational and Applied Mathematics}  \subtitle{}
\author[Juan Meza]{Juan Meza \\ School of Natural Sciences \\ University of California, Merced}
\date[Fall 2014]{Fall 2014}
\institute[UC Merced]

%\pgfdeclareimage[height=0.35cm]{UCM-logo}{UCM-logo}
% \logo{\pgfuseimage{UCM-logo}}

%% figures

\input{latex-defs}

\definecolor{navy}{RGB}{0,0,128}
\definecolor{forestgreen}{RGB}{34,139,34}
\definecolor{mylightsteelblue}{RGB}{176,196,222}
\definecolor{mylightcyan}{RGB}{180,255,255}
\definecolor{mypaleturquoise}{RGB}{175,238,238}
\definecolor{mylightgoldenrod}{RGB}{250,250,210}
\definecolor{mylightyellow}{RGB}{255,255,224}
\definecolor{mylightsalmon}{RGB}{255,160,122}

\setbeamercolor{blacklightsteelblue}{fg=black,bg=mylightsteelblue}
\setbeamercolor{blackpaleturquoise}{fg=black,bg=mypaleturquoise}
\setbeamercolor{blacklightcyan}{fg=black,bg=mylightcyan}
\setbeamercolor{blacklightgoldenrod}{fg=black,bg=mylightgoldenrod}
\setbeamercolor{blacklightyellow}{fg=black,bg=mylightyellow}
\setbeamercolor{blacklightsalmon}{fg=black,bg=mylightsalmon}
\setbeamercolor{blackcyan}{fg=black,bg=cyan}




\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\titlepage

} % end frame

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Consider the following matrix}

$$
K_n = \begin{bmatrix}
2          & -1        &  0 & 0 & \dots & 0 \\
-1      &     2       &  -1 & 0 & \dots &  0 \\
0          & -1      &  2 & -1 & \dots  &  0  \\
\dots  &  0          &  -1 & 2 & \ddots &  0  \\
0          & \dots          & 0 &  \ddots & 2   &  -1   \\
0       & 0          &  0 & 0 & -1     &   2 \\
\end{bmatrix}
$$
\begin{itemize}

\item We will encounter matrices of this form in future talks
\item We can take advantage of the special structure to improve our algorithms
\item This matrix is known as a Toeplitz matrix (constant diagonal)
\end{itemize}


} % end frame
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Another interesting matrix}

$$
D= \begin{bmatrix}
0          & 1/2        &  0 & 0 & \dots & -1/2 \\
-1/2      &     0       &  1/2 & 0 & \dots &  0 \\
0          & -1/2      &  0 & 1/2 & \dots  &  0  \\
\dots  &  0          &  -1/2 & 0 & \ddots &  0  \\
0          & \dots          & 0 &  \ddots & 0   &  1/2    \\
1/2       & 0          &  0 & 0 & -1/2     &   0 \\
\end{bmatrix}
$$

\begin{itemize}
\item A second order method can be given by the following matrix
\item This matrix is Toeplitz, skew-symmetrix, and circulant.  All very nice properties. 
\item Does this structure remind you of anything?
\end{itemize}

%\color{red} N.B.  The derivative is given by $w = \frac{1}{h} D \cdot u$
} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{More generally}

\begin{columns}[t]

    \column{.5\textwidth}
        \pgfputat{\pgfxy(0.0,-4.6)}{\pgfbox[left,base]{\pgfimage[width=\textwidth]{SparseMatrix.pdf}}}
    \column{.5\textwidth}
        \begin{definition}
        Any matrix where the majority of entries are zero is referred to as a {\it sparse} matrix.
        \end{definition}
        \begin{block}{Discussion}
        So what does {\it majority} mean?
        \end{block}
\end{columns}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Sparse methods}

\begin{itemize}
\item Sparse matrices arise in many science and engineering applications
\item Almost all discretization methods you will encounter will have a sparse structure
\item Sparse methods take advantage of special structure in matrices to improve computational efficiency
\end{itemize}
} % end frame

\section{SIM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Stationary Iterative Methods}

\begin{itemize}

\item Based on splitting techniques
\item Widely used in engineering, but have been superseded by more modern techniques
\item Still useful in certain contexts, e.g. preconditioners (more later)
\end{itemize}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Basic Stationary Iterative Method (SIM)}
Consider $Ax = b$

First we'll ``split" the matrix $A = M - N$, where $M = L + D$ and $U = -N$. In other words
$$
(L + D) x^{k+1} = b - Ux^{k}, \quad k=0, \ldots
$$
or
\begin{eqnarray}
M x^{k+1} &=& b + Nx^{k}, \quad k=0, \ldots \\
   x^{k+1} &=&  M^{-1}b + M^{-1}Nx^{k}, \\
   x^{k+1} &=&  Gx^{k} + c ,\label{eq:SIM}
\end{eqnarray}
where $G = M^{-1}N, c = M^{-1}b$.

\begin{block}{Note}
This is one of several splittings one can use.  Can you think of others?
\end{block}
} % end frame


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Useful Facts}


\begin{block}{Fact 1} $e^{k+1} = G e^{k}, \quad k=0, \ldots$.
\end{block}
\vspace{.2in}

\begin{block}{Fact 2}
If we use the SIM in equation~(\ref{eq:SIM}) $x^{k} \rightarrow \hat{x} \iff \rho(G) < 1$.
\end{block}
\vspace{.2in}

\begin{block}{Fact 3}
Many important matrices resulting from problems arising in CSE satisfy $\rho(G) < 1$.
\end{block}

} % end frame

\section{CG}

\frame{\frametitle{Discussion: Hestenes and Stiefel}
%
% Hestenes and Stiefel Paper
%
}
\section{Theory vs. Practice}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Finite Precision}

\begin{itemize}
\item In exact arithmetic the CG method will converge to the solution of $Ax=b$ in $n$ iterations.
\item In finite precision, roundoff error will contaminate the solution.
\item And even in the exact case, what does this mean for a system with dimension 1M or 1B?
\end{itemize}

} % end frame


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{CG Convergence Rate}
Can show that
\begin{equation}
|| x^{k} - \hat{x} ||_{A} \leq 2 \left ( \frac{\sqrt{\condA} - 1 }{ \sqrt{\condA}+ 1 } \right )^{k} || x^{0} - \hat{x} ||_{A}
\end{equation}

% Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   \topcaption{Number of iterations to reduce error by $10^{-3}$.} % requires the topcapt package
   \begin{tabular}{@{} llr @{}} % Column formatting, @{} suppresses leading/trailing space
      \toprule
      %\multicolumn{2}{c}{Convergence Rates} \\
      %\cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      $\condA$    & Value & iter \\
      \midrule
      1          & 0.0 & 1 \\
      $10^{2}$   & 0.82  &  38 \\
      $10^{4}$  & 0.98  &    376\\
      $10^{8}$  & 0.9998  &  38,000 \\
      \bottomrule
   \end{tabular}
   %\caption{}
   \label{tab:booktabs}
\end{table}
} % end frame


\section{Advanced Topic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Advanced Topic: Krylov subspace methods}
CG is an example of what we now call a Krylov space method.
\vspace{.25in}
\begin{definition}
A {\it Krylov} space is defined by:
$$
\mathcal{K}_k(A,b) = span \{b, Ab, A^2b, A^3b, \ldots, A^{k-1}b\}.
$$
\end{definition}
\vspace{.25in}
\begin{block}{Idea}
Choose your iterate, $x^{k}$, such that it belongs to the Krylov space and it minimizes the distance between  $x^{k}$ and $\hat{x}$.
\end{block}



} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Advanced Topic -- Preconditioning}


\begin{block}{Fundamental Idea}
If you don't like the problem you're given to solve, then change it into something that is easier to work with!
\end{block}
\vspace{.2in}
\begin{itemize}
\item If the error bound is dependent on the condition number can we do something about it?
\item Ideally what would we want the condition number of the problem to be?
\item Can we alter the problem to make it better conditioned, without changing the solution of the problem
\end{itemize}


} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Advanced Topic -- Preconditioning}
Consider $Ax = b$, where $\condA$ is large. Let  $M$ be a symmetric positive definite (spd) matrix. Then I can use CG to solve the equivalent problem:

\begin{equation}
M^{-1} A x^{k+1} = M^{-1} b, \quad k=0, \ldots
\end{equation}

and if $\kappa(M^{-1} A) < \condA$, I should be able to reduce the number of iterations.
\vspace{.25in}
\begin{block}{Preconditioning}
Choose $M^{-1}$ wisely and you will reduce the number of iterations significantly.  Choose poorly and all you've done is create more work for yourself!
\end{block}
\begin{block}{Exercise}
Note that in general $M^{-1}A$ may not be symmetric.  How then would you apply CG to this system? 
\end{block}
} % end frame



\section{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Summary}

\begin{itemize}
\item Sparse methods can be used to take advantage of special structure.
\item Iterative methods are the dominant form of solving large-scale systems of linear equations in modern day applications.
\item Most iterative methods must be preconditioned to be effective.
\end{itemize}

}% end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
  \frametitle<presentation>{References}    
  \begin{thebibliography}{10}    
  \beamertemplatebookbibitems
 \bibitem{GoVa96}
   Gene H. Golub and Charles F. Van Loan.
    \newblock {\em Matrix Computations, 3rd Ed}.
   \newblock Johns Hopkins, 1996.
    \beamertemplatearticlebibitems
  \beamertemplatebookbibitems
 \bibitem{Ke95}
   C. Tim Kelley.
    \newblock {\em Iterative Methods for Linear and Nonlinear Equations}.
   \newblock SIAM, 1995.
     \beamertemplatearticlebibitems
 \bibitem{HeSt52}
 Magnus R. Hestenes and Eduard Stiefel.
  \newblock Methods of Conjugate Gradients for Solving Linear Systems
 \newblock J. Res. of NBS, Vol. 49, No. 6, Dec. 1952.
 \end{thebibliography}

\end{frame}% end frame

\end{document}
