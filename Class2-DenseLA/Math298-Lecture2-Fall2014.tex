\documentclass[mathserif,handout]{beamer} % Math 298 Fall 2014
\usepackage{pdfsync} % works only on TeXShop, comment otherwise
\usepackage{pgf}
\usepackage{multimedia} % to embed movies in the PDF file
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{pgfpages}
\usepackage{topcapt}
\usepackage{booktabs}
\usepackage{biblatex}

\mode<presentation>
{
  \usetheme{Madrid}   %  \usetheme{Warsaw}
  % Note: this next command makes nice slides, but Adobe doesn't like it (you get blank pages)
  \setbeamertemplate{background canvas}[vertical shading][bottom=white,top=structure.fg!15]
  %\setbeamertemplate{footline}{\pgfuseimage{UCM-logo}}

  %\usecolortheme{seahorse}
  \usecolortheme{beaver}
 % \setbeamercovered{transparent}
}
%\usepackage[T1]{fontenc}
% For printing purposes only
%\pgfpagesuselayout{4 on 1}[letterpaper,border shrink=5mm,landscape]
\pgfpagesuselayout{resize to}[letterpaper,border shrink=5mm,landscape]  % print out full size on paper

\usepackage{amssymb,amsmath}
\hypersetup{%
 pdftitle={Math298 Lecture 2},
 pdfauthor={Juan Meza(UC Merced)},
 pdfsubject={Fundamental Concepts in Computational and Applied Mathematics},
 pdfkeywords={linear algebra, sparse, iterative}
 }
 
\title[Math 298 Lecture 2]{Fundamental Concepts in \\ Computational and Applied Mathematics}  \subtitle{}
\author[Juan Meza]{Juan Meza \\ School of Natural Sciences \\ University of California, Merced}
\date[Fall 2014]{Fall 2014}
\institute[UC Merced]

%\pgfdeclareimage[height=0.35cm]{UCM-logo}{UCM-logo}
% \logo{\pgfuseimage{UCM-logo}}

%% figures

%\input{latex-defs}
\newcommand {\condA}{\kappa(A)}
\newcommand {\condAi}{\kappa_{\infty}(A)}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\bigO}[1]{\mathcal{O}(#1)}

\definecolor{navy}{RGB}{0,0,128}
\definecolor{forestgreen}{RGB}{34,139,34}
\definecolor{mylightsteelblue}{RGB}{176,196,222}
\definecolor{mylightcyan}{RGB}{180,255,255}
\definecolor{mypaleturquoise}{RGB}{175,238,238}
\definecolor{mylightgoldenrod}{RGB}{250,250,210}
\definecolor{mylightyellow}{RGB}{255,255,224}
\definecolor{mylightsalmon}{RGB}{255,160,122}

\setbeamercolor{blacklightsteelblue}{fg=black,bg=mylightsteelblue}
\setbeamercolor{blackpaleturquoise}{fg=black,bg=mypaleturquoise}
\setbeamercolor{blacklightcyan}{fg=black,bg=mylightcyan}
\setbeamercolor{blacklightgoldenrod}{fg=black,bg=mylightgoldenrod}
\setbeamercolor{blacklightyellow}{fg=black,bg=mylightyellow}
\setbeamercolor{blacklightsalmon}{fg=black,bg=mylightsalmon}
\setbeamercolor{blackcyan}{fg=black,bg=cyan}




\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\titlepage

} % end frame

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Recap}

\begin{itemize}
\item Beware catastrophic cancellation -- know the limits of precision
\item Learn about the conditioning of your problem
\item Choose algorithms known to be stable so as to not introduce any more loss of precision than necessary
\begin{itemize}
	\item {\color{red} Conditioning} is fundamentally a characteristic of the problem while
	\item {\color{red} Stability} is related to algorithms
	\end{itemize}
\end{itemize}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Informally}

\begin{block}{Conditioning}
A problem is well conditioned if small changes in the input lead to small changes in the output. 
\end{block}

\begin{block}{Stability}
An algorithm is stable if it solves a "nearby" problem.
\end{block}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Basic Linear Algebra methods}

\begin{itemize}
\item Suppose $ x, y \in R^N$ and $A \in R^{N \times N}$
\item Familiar linear algebra operations
\begin{itemize}
\item dot products: $x^T y$
\item matvec: $Ax$
\item saxpy: $ax + y$
\item Solve $Ax=b$
\item Solve $Ax = \lambda x$
\end{itemize}

\item These operations have been immortalized into the {\sc BLAS} libraries
\end{itemize}
\begin{block}{Fundamental Fact of Computational Mathematics}
We cannot solve anything except linear systems
\end{block}

}
\frame{\frametitle{Methods for solving $Ax=b$}
\begin{itemize}
\item Various standard methods for solving systems of linear equations
\begin{itemize}
\item LU
\item Cholesky
\item QR
\item SVD
\end{itemize}
\item All have advantages and disadvantages -- learn about them!
\item Your choice will depend on the application, software availability, and time (\$\$) constraints
\end{itemize}
\begin{alertblock}{Warning}
Never (EVER) solve a linear system by calculating the inverse of the matrix and multiplying!
\end{alertblock}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Some Useful Terms to Know}

\begin{block}{Condition Number}
The condition number of a matrix A is given by: $ \condA = || A || \cdot || A^{-1} ||$
\end{block}

\begin{block}{Residual}
The residual of linear system is given by $b - Ax$
\end{block}

\begin{block}{Machine Precision}
Machine precision is denoted by $\mu$. Modern computers have $\mu \approx 10^{-16}.$
\end{block}

\begin{block}{Ill-conditioning}
A problem is said to be ill-conditioned if $\mu \cdot \condA \approx 1$
\end{block}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Fun Facts}

\begin{itemize}
\item Gaussian elimination always produces solutions with relatively small residuals (minor caveats).

\item Best possible error bound for solving a system of linear equations can be given by
\begin{equation}
\frac{|| x - \hat{x} ||_{\infty} }{|| x ||_{\infty}}  \leq 4 \mu \kappa_{\infty}(A),
\end{equation}
where $\mu$ is machine precision of the computer.

\end{itemize}
\begin{block}{Discussion}
If the residual of a computed solution is small, can I say that I have an "good" answer?  Can I trust it?
\end{block}
} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Stable or Unstable? Well conditioned or ill-conditioned?}
Consider $Ax = b$
where

$$
A = \begin{bmatrix}
0 & 1\\
1 & 0
\end{bmatrix}
$$

} % end frame


\section{Partial pivoting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{LU with Partial Pivoting (LU/PP)}

\begin{itemize}
\item Can show that LU/PP generates the exact solution to a perturbed problem $(A+E) x = b$, such that
\begin{equation*}
|| E ||_{\infty} \leq 8 n^3 \rho || A ||_{\infty} \mu.
\end{equation*}
\item {\em In theory}, the growth factor $\rho$ can grow exponentially, {\em but in practice} it is usually of order 10.
\item Consider the residual, $b - Ax$:
\begin{eqnarray*}
\norm{b - Ax}_{\infty} &=& \norm{Ex}_{\infty} \\
                           &\leq& 8 n^3 \rho \norm{A}_{\infty} \mu \norm{x}_\infty \\
                           &\approx&  \mu \norm{A}_\infty \norm{x}_\infty.
\end{eqnarray*}
\end{itemize}

} % end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Comparison of some familiar algorithms for $Ax=b$}
% Requires the booktabs if the memoir class is not being used
\begin{table}[htbp]
   \centering
   \topcaption{Familiar $Ax=b$ solution methods} % requires the topcapt package
   \begin{tabular}{@{} llll @{}} % Column formatting, @{} suppresses leading/trailing space
      \toprule
      %\multicolumn{2}{c}{Convergence Rates} \\
      %\cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
      Algorithm    & Work & Advantages & Disadvantages \\
      \midrule
      LU          & $\bigO{n^{3}}$   & simple  &  unstable \\
      LU w/PP & $\bigO{n^{3}}$   & (usually) stable &  growth factor; pivoting will change structure \\
      LU w/FP & $\bigO{n^{3}}$   & stable  &  more work than PP \\     
      QR         & $\bigO{n^{3}}$   & stable,no growth  &  2x work of LU \\    
      SVD       & $\bigO{n^{3}}$   & stable  &  more than 2x work \\    
      \bottomrule
   \end{tabular}
   %\caption{}
   \label{tab:booktabs}
\end{table}
} % end frame

\section{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{Summary}

\begin{itemize}
\item Basic Linear Algebra Subroutines ({\sc BLAS}) are at the core of many modern computer simulations.
\item Solution of linear systems is essential to your knowledge of computational mathematics.
\item Small residuals are not enough to show you have a good solution -- need to do a deeper analysis.
\end{itemize}

}% end frame

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{\frametitle{References}

\begin{itemize}
\item {\bf Matrix Computations, 3rd Ed.}, Gene H. Golub and Charles F. Van Loan, Johns Hopkins, 1996.
\end{itemize}

}% end frame

\end{document}
